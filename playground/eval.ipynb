{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load(output_path, target_path):\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        output = json.load(file)\n",
    "    with open(target_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        target = json.load(file)\n",
    "    print(len(output))\n",
    "    for o in output[:5]:\n",
    "        print(o)\n",
    "    print(len(target))\n",
    "    for t in target[:5]:\n",
    "        print(t)\n",
    "    return output, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_diff(output, target, diff_path):\n",
    "    count = 0\n",
    "    with open(diff_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for i in range(len(output)):\n",
    "            o = output[i]\n",
    "            t = target[i]\n",
    "            if o == t:\n",
    "                continue\n",
    "            count += 1\n",
    "            o_set = set()\n",
    "            t_set = set()\n",
    "            o_map = {}\n",
    "            t_map = {}\n",
    "            curr_len = 0\n",
    "            for j in range(len(o)):\n",
    "                start = curr_len\n",
    "                curr_len += len(o[j])\n",
    "                end = curr_len\n",
    "                o_set.add((start, end))\n",
    "                o_map[(start, end)] = j\n",
    "            curr_len = 0\n",
    "            for j in range(len(t)):\n",
    "                start = curr_len\n",
    "                curr_len += len(t[j])\n",
    "                end = curr_len\n",
    "                t_set.add((start, end))\n",
    "                t_map[(start, end)] = j\n",
    "            intersection = o_set.intersection(t_set)\n",
    "            o_diff_set = o_set - intersection\n",
    "            t_diff_set = t_set - intersection\n",
    "            o_diff_index = sorted([o_map[intv] for intv in o_diff_set])\n",
    "            t_diff_index = sorted([t_map[intv] for intv in t_diff_set])\n",
    "            o_diff = [o[j] for j in o_diff_index]\n",
    "            t_diff = [t[j] for j in t_diff_index]\n",
    "            file.write(str(i) + \"\\n\")\n",
    "            file.write(\"\".join(t) + \"\\n\")\n",
    "            file.write(str(o_diff) + \"\\n\")\n",
    "            file.write(str(t_diff) + \"\\n\")\n",
    "            file.write(\"\\n\")\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def char_level_eval(output, target):\n",
    "    # 0: S, 1: B, 2:I, 3:E\n",
    "    matrix = np.zeros((5, 5))\n",
    "    for i in range(len(output)):\n",
    "        o = output[i]\n",
    "        t = target[i]\n",
    "        o_chars = []\n",
    "        t_chars = []\n",
    "        for o_word in o:\n",
    "            if len(o_word) == 1:\n",
    "                o_chars.append(0)\n",
    "            else:\n",
    "                o_chars.append(1)\n",
    "                for _ in range(1, len(o_word) - 1):\n",
    "                    o_chars.append(2)\n",
    "                o_chars.append(3)\n",
    "        for t_word in t:\n",
    "            if len(t_word) == 1:\n",
    "                t_chars.append(0)\n",
    "            else:\n",
    "                t_chars.append(1)\n",
    "                for _ in range(1, len(t_word) - 1):\n",
    "                    t_chars.append(2)\n",
    "                t_chars.append(3)\n",
    "        for j in range(min(len(o_chars), len(t_chars))):\n",
    "            matrix[t_chars[j], o_chars[j]] += 1\n",
    "    for i in range(4):\n",
    "        matrix[i, 4] = matrix[i, i] / matrix[i, :4].sum()\n",
    "        matrix[4, i] = matrix[i, i] / matrix[:4, i].sum()\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "    precision = matrix[4, :4].mean()\n",
    "    recall = matrix[:4, 4].mean()\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(\"Character-level evaluation:\")\n",
    "    print(\"Precision: %.4f\" % precision)\n",
    "    print(\"Recall: %.4f\" % recall)\n",
    "    print(\"F1: %.4f\" % f1)\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_level_eval(output, target):\n",
    "    num_correct = 0\n",
    "    num_predicted = 0\n",
    "    num_truth = 0\n",
    "    for i in range(len(output)):\n",
    "        o = output[i]\n",
    "        t = target[i]\n",
    "        o_intervals = set()\n",
    "        t_intervals = set()\n",
    "        curr_len = 0\n",
    "        for j in range(len(o)):\n",
    "            start = curr_len\n",
    "            curr_len += len(o[j])\n",
    "            end = curr_len\n",
    "            o_intervals.add((start, end))\n",
    "        curr_len = 0\n",
    "        for j in range(len(t)):\n",
    "            start = curr_len\n",
    "            curr_len += len(t[j])\n",
    "            end = curr_len\n",
    "            t_intervals.add((start, end))\n",
    "        num_correct += len(o_intervals.intersection(t_intervals))\n",
    "        num_predicted += len(o_intervals)\n",
    "        num_truth += len(t_intervals)\n",
    "    precision = num_correct / num_predicted\n",
    "    recall = num_correct / num_truth\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(\"Word-level evaluation:\")\n",
    "    print(\"Precision: %.4f\" % precision)\n",
    "    print(\"Recall: %.4f\" % recall)\n",
    "    print(\"F1: %.4f\" % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# msr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3985\n",
      "['扬帆', '远东', '做', '与', '中国', '合作', '的', '先行']\n",
      "['希腊', '的', '经济', '结构', '较', '特殊', '。']\n",
      "['海运业', '雄踞', '全球', '之', '首', '，', '按', '吨位', '计', '占', '世界', '总数', '的', '１７％', '。']\n",
      "['另外', '旅游', '、', '侨汇', '也', '是', '经济', '收入', '的', '重要', '组成部分', '，', '制造业', '规模', '相对', '较', '小', '。']\n",
      "['多', '年', '来', '，', '中', '希', '贸易', '始终', '处于', '较', '低', '的', '水平', '，', '希腊', '几乎', '没有', '在', '中国', '投资', '。']\n",
      "3985\n",
      "['扬帆', '远东', '做', '与', '中国', '合作', '的', '先行']\n",
      "['希腊', '的', '经济', '结构', '较', '特殊', '。']\n",
      "['海运', '业', '雄踞', '全球', '之', '首', '，', '按', '吨位', '计', '占', '世界', '总数', '的', '１７％', '。']\n",
      "['另外', '旅游', '、', '侨汇', '也是', '经济', '收入', '的', '重要', '组成部分', '，', '制造业', '规模', '相对', '较小', '。']\n",
      "['多年来', '，', '中', '希', '贸易', '始终', '处于', '较低', '的', '水平', '，', '希腊', '几乎', '没有', '在', '中国', '投资', '。']\n"
     ]
    }
   ],
   "source": [
    "msr_output_path = \"D:\\\\Local\\\\Workspace\\\\CSE256\\\\AdaSeq\\\\playground\\\\outputs\\\\msr_output.json\"\n",
    "msr_target_path = \"D:\\\\Local\\\\Workspace\\\\CSE256\\\\AdaSeq\\\\playground\\\\outputs\\\\msr_target.json\"\n",
    "msr_output, msr_target = load(msr_output_path, msr_target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3148\n"
     ]
    }
   ],
   "source": [
    "msr_diff_path = \"D:\\\\Local\\\\Workspace\\\\CSE256\\\\AdaSeq\\\\playground\\\\diffs\\\\diff_msr.txt\"\n",
    "save_diff(msr_output, msr_target, msr_diff_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level evaluation:\n",
      "Precision: 0.8765\n",
      "Recall: 0.8276\n",
      "F1: 0.8514\n",
      "[[43835.      1600.       388.      2261.         0.9116]\n",
      " [ 2946.     55318.       477.        40.         0.9411]\n",
      " [  483.      4136.      9878.      4204.         0.5282]\n",
      " [ 2961.        93.      1085.     54642.         0.9296]\n",
      " [    0.8728     0.9047     0.8351     0.8936     0.    ]]\n",
      "Word-level evaluation:\n",
      "Precision: 0.8516\n",
      "Recall: 0.8876\n",
      "F1: 0.8692\n"
     ]
    }
   ],
   "source": [
    "char_level_eval(msr_output, msr_target)\n",
    "word_level_eval(msr_output, msr_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1938\n",
      "['共同', '创造', '美好', '的', '新', '世纪', '——', '二○○一年', '新年', '贺词']\n",
      "['（', '二○○○年', '十二月', '三十一日', '）', '（', '附', '图片', '1', '张', '）']\n",
      "['女士', '们', '，', '先生', '们', '，', '同志', '们', '，', '朋友', '们', '：']\n",
      "['2001年', '新年', '钟声', '即将', '敲响', '。', '人类', '社会', '前进', '的', '航船', '就要', '驶入', '21', '世纪', '的', '新', '航程', '。', '中国', '人民', '进入', '了', '向', '现代化', '建设', '第三', '步', '战略', '目标', '迈进', '的', '新', '征程', '。']\n",
      "['在', '这个', '激动人心', '的', '时刻', '，', '我', '很', '高兴', '通过', '中国', '国际', '广播', '电台', '、', '中央', '人民', '广播', '电台', '和', '中央', '电视台', '，', '向', '全国', '各族', '人民', '，', '向', '香港', '特别', '行政区', '同胞', '、', '澳门', '特别', '行政区', '同胞', '和', '台湾', '同胞', '、', '海外', '侨胞', '，', '向', '世界', '各国', '的', '朋友', '们', '，', '致以', '新', '世纪', '第一', '个', '新年', '的', '祝贺', '！']\n",
      "1938\n",
      "['共同', '创造', '美好', '的', '新', '世纪', '——', '二○○一年', '新年', '贺词']\n",
      "['（', '二○○○年', '十二月', '三十一日', '）', '（', '附', '图片', '1', '张', '）']\n",
      "['女士', '们', '，', '先生', '们', '，', '同志', '们', '，', '朋友', '们', '：']\n",
      "['2001年', '新年', '钟声', '即将', '敲响', '。', '人类', '社会', '前进', '的', '航船', '就要', '驶入', '21', '世纪', '的', '新', '航程', '。', '中国', '人民', '进入', '了', '向', '现代化', '建设', '第三', '步', '战略', '目标', '迈进', '的', '新', '征程', '。']\n",
      "['在', '这个', '激动人心', '的', '时刻', '，', '我', '很', '高兴', '通过', '中国', '国际', '广播', '电台', '、', '中央', '人民', '广播', '电台', '和', '中央', '电视台', '，', '向', '全国', '各族', '人民', '，', '向', '香港', '特别', '行政区', '同胞', '、', '澳门', '特别', '行政区', '同胞', '和', '台湾', '同胞', '、', '海外', '侨胞', '，', '向', '世界', '各国', '的', '朋友', '们', '，', '致以', '新', '世纪', '第一', '个', '新年', '的', '祝贺', '！']\n"
     ]
    }
   ],
   "source": [
    "pku_output_path = \"D:\\\\Local\\\\Workspace\\\\CSE256\\\\AdaSeq\\\\playground\\\\outputs\\\\pku_output.json\"\n",
    "pku_target_path = \"D:\\\\Local\\\\Workspace\\\\CSE256\\\\AdaSeq\\\\playground\\\\outputs\\\\pku_target.json\"\n",
    "pku_output, pku_target = load(pku_output_path, pku_target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "969\n"
     ]
    }
   ],
   "source": [
    "pku_diff_path = \"D:\\\\Local\\\\Workspace\\\\CSE256\\\\AdaSeq\\\\playground\\\\diffs\\\\diff_pku.txt\"\n",
    "save_diff(pku_output, pku_target, pku_diff_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level evaluation:\n",
      "Precision: 0.9564\n",
      "Recall: 0.9707\n",
      "F1: 0.9635\n",
      "[[44633.       752.       281.       844.         0.9596]\n",
      " [  498.     54501.       518.        30.         0.9812]\n",
      " [   65.       198.     10731.       164.         0.9617]\n",
      " [  459.        35.       605.     54448.         0.9802]\n",
      " [    0.9776     0.9822     0.8843     0.9813     0.    ]]\n",
      "Word-level evaluation:\n",
      "Precision: 0.9727\n",
      "Recall: 0.9639\n",
      "F1: 0.9683\n"
     ]
    }
   ],
   "source": [
    "char_level_eval(pku_output, pku_target)\n",
    "word_level_eval(pku_output, pku_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nlpcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2052\n",
      "['【', '神秘', '中国', '财团', '本周', '或', '买', '下', 'AC', '米兰', '，', '代价', '超', '10亿', '欧元', '】']\n",
      "['【', '缩量', '回调', '，', '3000', '点', '还要', '徘徊', '多久', '？', '】']\n",
      "['【', '美国', '再', '启动', '337', '调查', '：', '联想', '中兴', '或', '遭', '重拳', '】']\n",
      "['【', '联盛', '破产', '冲击波', '：', '山西', '宏特', '资金链', '断裂', '，', '债委会', '难', '救', '煤焦油', '龙头', '】']\n",
      "['【', '自由', '教师', '：', '年收入', '过', '百万', '的', '新', '工作', '？', '】']\n",
      "2052\n",
      "['【', '神秘', '中国', '财团', '本周', '或', '买下', 'AC米兰', '，', '代价', '超', '10亿', '欧元', '】']\n",
      "['【', '缩量', '回调', '，', '3000', '点', '还要', '徘徊', '多久', '？', '】']\n",
      "['【', '美国', '再', '启动', '337', '调查', '：', '联想', '中兴', '或', '遭', '重拳', '】']\n",
      "['【', '联盛', '破产', '冲击波', '：', '山西', '宏特', '资金链', '断裂', '，', '债委会', '难救', '煤焦油', '龙头', '】']\n",
      "['【', '自由', '教师', '：', '年收入', '过', '百万', '的', '新', '工作', '？', '】']\n"
     ]
    }
   ],
   "source": [
    "nlpcc_output_path = \"D:\\\\Local\\\\Workspace\\\\CSE256\\\\AdaSeq\\\\playground\\\\outputs\\\\nlpcc_output.json\"\n",
    "nlpcc_target_path = \"D:\\\\Local\\\\Workspace\\\\CSE256\\\\AdaSeq\\\\playground\\\\outputs\\\\nlpcc_target.json\"\n",
    "nlpcc_output, nlpcc_target = load(nlpcc_output_path, nlpcc_target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346\n"
     ]
    }
   ],
   "source": [
    "nlpcc_diff_path = \"D:\\\\Local\\\\Workspace\\\\CSE256\\\\AdaSeq\\\\playground\\\\diffs\\\\diff_nlpcc.txt\"\n",
    "save_diff(nlpcc_output, nlpcc_target, nlpcc_diff_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level evaluation:\n",
      "Precision: 0.9245\n",
      "Recall: 0.9186\n",
      "F1: 0.9215\n",
      "[[19310.       576.        64.       578.         0.9407]\n",
      " [ 1292.     21590.       264.        23.         0.9318]\n",
      " [  111.       408.      5527.       330.         0.8668]\n",
      " [ 1214.        23.       266.     21666.         0.9351]\n",
      " [    0.8806     0.9554     0.903      0.9588     0.    ]]\n",
      "Word-level evaluation:\n",
      "Precision: 0.9061\n",
      "Recall: 0.9232\n",
      "F1: 0.9146\n"
     ]
    }
   ],
   "source": [
    "char_level_eval(nlpcc_output, nlpcc_target)\n",
    "word_level_eval(nlpcc_output, nlpcc_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# medical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "907\n",
      "['她', '现在', '85', '岁', '了', '，', '生活', '能', '自理', '，', '自己', '能', '做饭', '，', '能', '骑', '电瓶车', '，', '今天', '有', '剧烈', '咳嗽', '，', '有', '痰']\n",
      "['5', '年', '前', '感冒', '咳嗽', '有', '半', '年', '时间', '服', '止咳', '糖浆', '等', '！']\n",
      "['半', '年', '后', '腿', '肿', '了', '血压', '高', '140', '！']\n",
      "['住院', '拍', '肺部', 'CT', '显示', '肺栓塞', '！']\n",
      "['现在', '走路', '快', '了', '就', '气喘', '！']\n",
      "907\n",
      "['她', '现在', '85', '岁', '了', '，', '生活', '能', '自理', '，', '自己', '能', '做饭', '，', '能', '骑', '电瓶车', '，', '今天', '有', '剧烈', '咳嗽', '，', '有', '痰']\n",
      "['5', '年', '前', '感冒', '咳嗽', '有', '半年', '时间', '服', '止咳', '糖浆', '等', '！']\n",
      "['半年', '后', '腿', '肿', '了', '血压', '高', '140', '！']\n",
      "['住院', '拍', '肺部', 'CT', '显示', '肺栓塞', '！']\n",
      "['现在', '走路', '快', '了', '就', '气喘', '！']\n"
     ]
    }
   ],
   "source": [
    "medical_output_path = \"D:\\\\Local\\\\Workspace\\\\CSE256\\\\AdaSeq\\\\playground\\\\outputs\\\\medical_output.json\"\n",
    "medical_target_path = \"D:\\\\Local\\\\Workspace\\\\CSE256\\\\AdaSeq\\\\playground\\\\outputs\\\\medical_target.json\"\n",
    "medical_output, medical_target = load(medical_output_path, medical_target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612\n"
     ]
    }
   ],
   "source": [
    "medical_diff_path = \"D:\\\\Local\\\\Workspace\\\\CSE256\\\\AdaSeq\\\\playground\\\\diffs\\\\diff_medical.txt\"\n",
    "save_diff(medical_output, medical_target, medical_diff_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level evaluation:\n",
      "Precision: 0.8174\n",
      "Recall: 0.7910\n",
      "F1: 0.8040\n",
      "[[5604.      326.       16.      407.        0.8821]\n",
      " [ 804.     5800.      119.       20.        0.8602]\n",
      " [ 145.      221.      741.      195.        0.5691]\n",
      " [ 773.       25.      195.     5750.        0.8527]\n",
      " [   0.7649    0.9102    0.6919    0.9024    0.    ]]\n",
      "Word-level evaluation:\n",
      "Precision: 0.8081\n",
      "Recall: 0.8453\n",
      "F1: 0.8263\n"
     ]
    }
   ],
   "source": [
    "char_level_eval(medical_output, medical_target)\n",
    "word_level_eval(medical_output, medical_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
